{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e7bf50b79fbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A001'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'C022'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_filter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_filter = df['ID'].isin(['A001','C022',...])\n",
    "df[df_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Search():\n",
    "    global best_model, saved_moldel\n",
    "    \n",
    "    param_grid = {\"n_estimators\": range(20, 100, 2),\n",
    "                  \"max_depth\": range(4, 50, 2),\n",
    "                  \"min_samples_leaf\": range(2, 100, 2),\n",
    "                  \"max_features\": sp_randint(1, n_features),\n",
    "                  \"min_samples_split\": sp_randint(2, 10),\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "    clf = RandomForestClassifier(class_weight = 'balanced')\n",
    "    n_iter_search = 500\n",
    "    estimator = RandomizedSearchCV(clf,\n",
    "                                   param_distributions = param_grid,\n",
    "                                   n_iter = n_iter_search,\n",
    "                                   scoring = 'roc_auc',\n",
    "                                   verbose = 0,\n",
    "                                   n_jobs = 1)\n",
    "        \n",
    "    fit = estimator.fit(X_train, y_train)\n",
    "\n",
    "    # Cross validation with 20 iterations to get smoother mean test and train\n",
    "    # score curves, each time with 20% data randomly selected as a validation set.\n",
    "    cv_ = ShuffleSplit(n_splits = 20, test_size = 0.20, random_state = 0)\n",
    "    Plot_learning_curve(estimator, \n",
    "                        'Learning Curves',\n",
    "                        X_train, y_train, \n",
    "                        cv = cv_,\n",
    "                        n_jobs = 1)\n",
    "     \n",
    "    Report_scores(estimator.cv_results_, n_top = 3)\n",
    "    \n",
    "    best_model = estimator.best_estimator_\n",
    "    print('\\nbest_model:\\n', best_model)\n",
    "\n",
    "    print('\\nFeature Importances:', best_model.feature_importances_)\n",
    "    Plot_predictor_importance(best_model, feature_columns)\n",
    "\n",
    "    y_predicted = best_model.predict(X_train)\n",
    "    probabilities = best_model.predict_proba(X_train)\n",
    "\n",
    "    c_report = classification_report(y_train, y_predicted)\n",
    "    print('\\nClassification report:\\n', c_report)\n",
    "\n",
    "    y_predicted_train = best_model.predict(X_train)\n",
    "    cm = confusion_matrix(y_train, y_predicted_train)\n",
    "    auc = roc_auc_score(y_train, y_predicted_train)\n",
    "    Print_confusion_matrix(cm, auc, 'Confusion matrics of the training dataset')\n",
    "\n",
    "    y_predicted = best_model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "    auc = roc_auc_score(y_test, y_predicted)\n",
    "\n",
    "    ntotal = len(y_test)\n",
    "    correct = y_test == y_predicted\n",
    "    numCorrect = sum(correct)\n",
    "    percent = round( (100.0*numCorrect)/ntotal, 6)\n",
    "    print(\"\\nCorrect classifications on test data: {0:d}/{1:d} {2:8.3f}%\".format(numCorrect, ntotal, percent))\n",
    "    prediction_score = 100.0*best_model.score(X_test, y_test)\n",
    "    print('Random Forest Prediction Score on test data: %8.3f' % prediction_score)\n",
    "\n",
    "    model_path = 'C:/sm/BottleRockets/trained_models/sklearn_rf_classify.pkl'\n",
    "    joblib.dump(best_model, model_path)\n",
    "\n",
    "    saved_model = joblib.load(model_path)\n",
    "    y_predicted_test = best_model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predicted_test)\n",
    "    auc = roc_auc_score(y_test, y_predicted_test)\n",
    "    Print_confusion_matrix(cm, auc, 'Confusion matrics of the test dataset')\n",
    "    ROC_Curve(best_model, auc)\n",
    "    Print_Metrics(saved_model)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_confusion_matrix(cm, auc, heading):\n",
    "    print('\\n', heading)\n",
    "    print(cm)\n",
    "    true_negative  = cm[0,0]\n",
    "    true_positive  = cm[1,1]\n",
    "    false_negative = cm[1,0]\n",
    "    false_positive = cm[0,1]\n",
    "    total = true_negative + true_positive + false_negative + false_positive\n",
    "    accuracy = (true_positive + true_negative)/total\n",
    "    precision = (true_positive)/(true_positive + false_positive)\n",
    "    recall = (true_positive)/(true_positive + false_negative)\n",
    "    misclassification_rate = (false_positive + false_negative)/total\n",
    "    F1 = (2*true_positive)/(2*true_positive + false_positive + false_negative)\n",
    "    print('accuracy.................%7.4f' % accuracy)\n",
    "    print('precision................%7.4f' % precision)\n",
    "    print('recall...................%7.4f' % recall)\n",
    "    print('F1.......................%7.4f' % F1)\n",
    "    print('auc......................%7.4f' % auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot the learning curves\n",
    "\n",
    "def Plot_learning_curve(estimator, title, X, y, ylim = None, cv = None,\n",
    "                        n_jobs = 1, train_sizes = np.linspace(0.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, \n",
    "                                                            X, y,\n",
    "                                                            cv = cv,\n",
    "                                                            n_jobs = n_jobs,\n",
    "                                                            train_sizes = train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_confusion_matrix(cm, auc, heading):\n",
    "    print('\\n', heading)\n",
    "    print(cm)\n",
    "    true_negative  = cm[0,0]\n",
    "    true_positive  = cm[1,1]\n",
    "    false_negative = cm[1,0]\n",
    "    false_positive = cm[0,1]\n",
    "    total = true_negative + true_positive + false_negative + false_positive\n",
    "    accuracy = (true_positive + true_negative)/total\n",
    "    precision = (true_positive)/(true_positive + false_positive)\n",
    "    recall = (true_positive)/(true_positive + false_negative)\n",
    "    misclassification_rate = (false_positive + false_negative)/total\n",
    "    F1 = (2*true_positive)/(2*true_positive + false_positive + false_negative)\n",
    "    print('accuracy.................%7.4f' % accuracy)\n",
    "    print('precision................%7.4f' % precision)\n",
    "    print('recall...................%7.4f' % recall)\n",
    "    print('F1.......................%7.4f' % F1)\n",
    "    print('auc......................%7.4f' % auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_Metrics(saved_rf):\n",
    "    print('\\nModel performance on the test data set:')\n",
    "\n",
    "    # print('Train Accuracy.......', accuracy_score(y_train, best_model.predict(X_train)))\n",
    "    # print('Validate Accuracy....', accuracy_score(y_valid, best_model.predict(X_valid)))\n",
    "\n",
    "    y_predict_test  = best_model.predict(X_test)\n",
    "    mse             = metrics.mean_squared_error(y_test, y_predict_test)\n",
    "    logloss_test    = metrics.log_loss(y_test, y_predict_test)\n",
    "    accuracy_test   = metrics.accuracy_score(y_test, y_predict_test)\n",
    "    accuracy_test2  = best_model.score(X_test, y_test)\n",
    "    F1_test         = metrics.f1_score(y_test, y_predict_test)\n",
    "    precision_test  = precision_score(y_test, y_predict_test, average='binary')\n",
    "    precision_test2 = metrics.precision_score(y_test, y_predict_test)\n",
    "    recall_test     = recall_score(y_test, y_predict_test, average='binary')\n",
    "    auc_test        = metrics.roc_auc_score(y_test, y_predict_test)\n",
    "    r2_test         = metrics.r2_score(y_test, y_predict_test)\n",
    "   \n",
    "    #test_auc       = h2o.get_model(\"best_rf\").model_performance(test_data=test).auc()\n",
    "    #print('Best model performance based on auc: ', test_auc)\n",
    "    \n",
    "    header = [\"Metric\", \"Test\"]\n",
    "    table  = [\n",
    "               [\"logloss\",   logloss_test],\n",
    "               [\"accuracy\",  accuracy_test],\n",
    "               [\"precision\", precision_test],\n",
    "               [\"F1\",        F1_test],\n",
    "               [\"r2\",        r2_test],\n",
    "               [\"AUC\",       auc_test]\n",
    "             ]\n",
    "\n",
    "    print(tabulate(table, header, tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_Curve(rf, auc):\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    rf_fit = rf.fit(X_train, y_train)\n",
    "    fit = one_hot_encoder.fit(rf.apply(X_train))\n",
    "    y_predicted = rf.predict_proba(X_test)[:, 1]\n",
    "    false_positive, true_positive, _ = roc_curve(y_test, y_predicted)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(false_positive, true_positive, color='darkorange', label='Random Forest')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve (area = %0.2f)' % auc)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correlation_plot(df):\n",
    "    plt.ioff()\n",
    "    red_green = [\"#ff0000\", \"#00ff00\"]\n",
    "    sns.set_palette(red_green)\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    g = sns.pairplot(df,\n",
    "                     diag_kind = 'kde',\n",
    "                     hue = 'Altitude',\n",
    "                     markers = [\"o\", \"D\"],\n",
    "                     size = 1.5,\n",
    "                     aspect = 1,\n",
    "                     plot_kws = {\"s\": 6})\n",
    "    g.fig.subplots_adjust(right = 0.9)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time taken in grid search: {0: .2f}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "  def __init__(self):\n",
    "    self.start = time.time()\n",
    "\n",
    "  def restart(self):\n",
    "    self.start = time.time()\n",
    "\n",
    "  def get_time(self):\n",
    "    end = time.time()\n",
    "    m, s = divmod(end - self.start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "    return time_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Operating system version....', platform.platform())\n",
    "print(\"Python version is........... %s.%s.%s\" % sys.version_info[:3])\n",
    "print('scikit-learn version is.....', sklearn.__version__)\n",
    "print('pandas version is...........', pd.__version__)\n",
    "print('numpy version is............', np.__version__)\n",
    "print('matplotlib version is.......', matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error, cohen_kappa_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve, SCORERS\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.externals import joblib\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.copy()\n",
    "train_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\n",
    "train_data[\"Embarked\"].fillna(train_df['Embarked'].value_counts().idxmax(), inplace=True)\n",
    "train_data.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "ax = train_df[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\n",
    "train_df[\"Age\"].plot(kind='density', color='teal')\n",
    "ax = train_data[\"Age\"].hist(bins=15, density=True, stacked=True, color='orange', alpha=0.5)\n",
    "train_data[\"Age\"].plot(kind='density', color='orange')\n",
    "ax.legend(['Raw Age', 'Adjusted Age'])\n",
    "ax.set(xlabel='Age')\n",
    "plt.xlim(-10,85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_coefs(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "    lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name)\n",
    "                                   for coef, name in lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ridge_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5d9c2e13140b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Ridge model:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretty_print_coefs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mridge_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ridge_reg' is not defined"
     ]
    }
   ],
   "source": [
    "print (\"Ridge model:\", pretty_print_coefs(ridge_reg.params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(d['agekd_binomial'], d['premarsx_binomial'], normalize='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    ((d['PREMARSX'] == 1) | (d['PREMARSX'] == 2)),\n",
    "    ((d['PREMARSX'] == 3) | (d['PREMARSX'] == 4))]\n",
    "choices = [1,0]\n",
    "d['premarsx_binomial'] = np.select(conditions, choices, default=np.nan)\n",
    "d['premarsx_binomial'].value_counts()\n",
    "\n",
    "\n",
    "conditions = [\n",
    "    ((dat['relig'] == 1) | (dat['relig'] == 2) | (dat['relig'] == 11)) ,\n",
    "     (dat['relig'] == 3), \n",
    "     (dat['relig'] == 6), \n",
    "     (dat['relig'] == 9),\n",
    "     ((dat['relig'] != 1) & ((dat['relig'] != 2)) & (dat['relig'] != 11) & (dat['relig'] != 3) & \n",
    "    (dat['relig'] != 6) & (dat['relig'] != 9))\n",
    "]\n",
    "category = ['Christian', 'Jewish', 'Buddhism', 'Muslim', 'others'] #Christian, Jewish, Buddhism, Muslim, and others\n",
    "dat['relig_new'] = np.select(conditions, category, default=np.nan)\n",
    "\n",
    "dat.relig_new.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = np.exp(result.params)\n",
    "ci = np.exp(result.conf_int())\n",
    "ci.columns = [\"2.5%\", \"97.5%\"]\n",
    "ci['exp'] = exp\n",
    "ci['odds %'] = round((exp - 1), 2)*100\n",
    "ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fd = FirstDifferenceOLS.from_formula(formula2, dat2)\n",
    "res_fd = lm_fd.fit(cov_type='clustered', cluster_entity=True)\n",
    "print(res_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agekd_binomial_tab = pd.crosstab(index=d[\"agekd_binomial\"],  # Make a crosstab\n",
    "                              columns=\"count\")     \n",
    "\n",
    "def compute_percentage(x):\n",
    "      pct = float(x/agekd_binomail_tab['count'].sum()) * 100\n",
    "      return round(pct, 2)\n",
    "\n",
    "agekd_binomial_tab['%'] = agekd_binomial_tab.apply(compute_percentage, axis=1)\n",
    "\n",
    "agekd_binomial_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cut them into age basket ##\n",
    "d['agekd_cut'] = pd.cut(d['AGEKDBRN'], bins=[12,20,30,40,50], labels=None, include_lowest= True)\n",
    "print (pd.value_counts(d[\"agekd_cut\"], sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = d[['AGEKDBRN','ID']].groupby('AGEKDBRN').count()\n",
    "bb['%'] = bb/bb.ID.sum()*100\n",
    "bb = bb.rename(columns = {'ID' : 'Total'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.crosstab(d.SIBS, d.sib_cut)\n",
    "res.astype('float').div(res.sum(axis=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sib_cuts.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.filter(like='age').columns\n",
    "\n",
    "aa = dat.filter(like = \"age\").columns\n",
    "\n",
    "for x in aa: \n",
    "    print(dat[x].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs[['manner','sex','ses','immigrant','education','live with parents','privacy','marital', 'age']].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvs['manner']=7-(wvs['manner']) ## reverse respond : higher variable means more mannered ... ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvssub = pd.concat([wvs_new.manner, wvs_new.age, wvs_new.citizen, wvs_new.education, wvs_new.parenting, wvs_new.secret_survey, wvs_new.female, wvs_new.married, wvs_new.V2, \n",
    "                   pd.get_dummies(wvs_new.ses.astype(int), prefix='ses')],  axis = 1 )\n",
    "wvssub.columns\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = smf.ols(formula = f, \n",
    "              data = wvssub, \n",
    "              subset = (wvssub['V2']==840)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_ln = smf.ols(formula = \"ln_agewed ~ ln_realrinc\", data = dat).fit(cov_type='HC3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial F-test\n",
    "sm.stats.anova_lm(lm1, lm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try dummy variable this time\n",
    "lm1_dummy = smf.ols(formula = 'Neighbors ~ C(Robberies) + C(Alcohol_consumption) + C(Police_deprive) + C(Racist) + C(Drug_sale)', data = wvs_new, subset= (wvs_new['Country'] == 840)).fit()\n",
    "print (lm1_dummy.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sub_dat[\"age\"], lm_interaction.params[0] + lm_interaction.params[1] * 1 + lm_interaction.params[2] * sub_dat[\"age\"] + lm_interaction.params[3] * 1 * sub_dat[\"age\"], 'b', label = 'Egypt', alpha = 0.9)\n",
    "plt.plot(sub_dat[\"age\"], lm_interaction.params[0] + lm_interaction.params[1] * 0 + lm_interaction.params[2] * sub_dat[\"age\"] + lm_interaction.params[3] * 0 * sub_dat[\"age\"], 'r', label = 'America', alpha = 0.9)\n",
    "plt.title(\"Reading a newspaper, America (red) vs. Egypt (blue)\")\n",
    "plt.xlabel(\"age\")\n",
    "plt.ylabel(\"Reading newspaper\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(degree_lm.resid, color = \"skyblue\", ec=\"black\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_glm = smf.glm(formula=formula, data=sub, family = sm.families.Binomial())\n",
    "result = degree_glm.fit()\n",
    "print(result.summary())\n",
    "\n",
    "\n",
    "degree_logit = sm.formula.logit(formula = formula, data = sub).fit()\n",
    "print (degree_logit.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit2prob (logit):\n",
    "    odds = np.exp(logit)\n",
    "    prob = odds / (1 + odds) \n",
    "    return(prob);\n",
    "\n",
    "\n",
    "intercept = degree_logit.params.Intercept\n",
    "b_joblose = degree_logit.params.joblose\n",
    "b_jobfind = degree_logit.params.jobfind\n",
    "b_satjob = degree_logit.params.satjob\n",
    "b_satfin = degree_logit.params.satfin\n",
    "b_finalter = degree_logit.params.finalter\n",
    "b_income06 = degree_logit.params.income06\n",
    "b_class_1 = degree_logit.params.class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person A's predicted prob model:\n",
    "\n",
    "'''\n",
    "A does not think that he/she will lose his/her job (4),\n",
    "-- A thought that finding a job was very easy (3),\n",
    "-- A is very satisfied with the current position (4) and financial status (4),\n",
    "-- A thinks that his/her financial status is improving (3),\n",
    "-- A's income falls at the top tier (25),\n",
    "-- A's social class is the upper class (4)\n",
    "'''\n",
    "\n",
    "logits_degree1 = intercept + (4 * b_joblose) + (3 * b_jobfind) + (4 * b_satjob) + (3 * b_satfin) + (3 * b_finalter) + (25 * b_income06) + (4 * b_class_1)\n",
    "logit2prob(logits_degree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrape_tab = pd.crosstab(index = dat['abrape'], columns = 'count')\n",
    "abrape_tab['%'] = 100 * abrape_tab / abrape_tab.sum()\n",
    "abrape_tab['cum_%'] = 100 * abrape_tab['count'].cumsum()/abrape_tab['count'].sum()\n",
    "abrape_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.loc[dat['educ'] == 98] = np.nan\n",
    "dat.loc[dat['educ'] == 99] = np.nan\n",
    "dat = dat.dropna(subset = ['educ'])\n",
    "plt.hist(dat.educ, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85, align = 'mid')\n",
    "plt.title(\"Distribution of Education\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot out the realinc distribtuion \n",
    "dat = dat.dropna(subset = ['realinc'])\n",
    "import seaborn as sns\n",
    "sns.distplot(dat.realinc).set_title(\"Distribution of family real income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
