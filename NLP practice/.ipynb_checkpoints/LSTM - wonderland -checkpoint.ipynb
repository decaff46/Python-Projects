{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-22\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "print(date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alice in Wonderland NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.840B.300d.txt', 'test.zip', 'train.zip', 'wonderland.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.listdir('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/wonderland.txt'\n",
    "\n",
    "raw_txt = open(file=file, encoding='utf-8').read()\n",
    "raw_txt = raw_txt.lower()\n",
    "#raw_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_txt)))\n",
    "char_to_int = dict((c,i) for i,c in enumerate(chars))\n",
    "\n",
    "#chars_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163780\n",
      "Total Vocabs:  58\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_txt)\n",
    "n_vocab = len(chars)\n",
    "\n",
    "print ('Total Characters: ', n_chars)\n",
    "print ('Total Vocabs: ', n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163680\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100 \n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(0,(n_chars - seq_length),1):\n",
    "    seq_in = raw_txt[i:i+seq_length]\n",
    "    seq_out = raw_txt[i+seq_length]\n",
    "    \n",
    "    x.append([char_to_int[ch] for ch in seq_in])\n",
    "    y.append(char_to_int[seq_out])\n",
    "    \n",
    "n_patterns = len(x)\n",
    "print('Total Patterns: ', n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "import numpy as np \n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(x, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalizing\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single hidden LSTM layer with 256 memory units\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape= (X.shape[1], X.shape[2])), \n",
    "    Dropout(0.2), \n",
    "    Dense(y.shape[1], activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 58)                14906     \n",
      "=================================================================\n",
      "Total params: 279,098\n",
      "Trainable params: 279,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'weights_improvement_{epoch: 02d}_{loss:.4f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath = filepath, \n",
    "                            monitor = 'loss', \n",
    "                            verbose =1, \n",
    "                            save_best_only=True, \n",
    "                            mode = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "163680/163680 [==============================] - 692s 4ms/step - loss: 2.9834\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.98345, saving model to weights_improvement_ 1_2.9834.hdf5\n",
      "Epoch 2/20\n",
      "163680/163680 [==============================] - 684s 4ms/step - loss: 2.8038\n",
      "\n",
      "Epoch 00002: loss improved from 2.98345 to 2.80381, saving model to weights_improvement_ 2_2.8038.hdf5\n",
      "Epoch 3/20\n",
      "163680/163680 [==============================] - 685s 4ms/step - loss: 2.7255\n",
      "\n",
      "Epoch 00003: loss improved from 2.80381 to 2.72547, saving model to weights_improvement_ 3_2.7255.hdf5\n",
      "Epoch 4/20\n",
      "163680/163680 [==============================] - 693s 4ms/step - loss: 2.6608\n",
      "\n",
      "Epoch 00004: loss improved from 2.72547 to 2.66079, saving model to weights_improvement_ 4_2.6608.hdf5\n",
      "Epoch 5/20\n",
      "163680/163680 [==============================] - 687s 4ms/step - loss: 2.6092\n",
      "\n",
      "Epoch 00005: loss improved from 2.66079 to 2.60917, saving model to weights_improvement_ 5_2.6092.hdf5\n",
      "Epoch 6/20\n",
      "163680/163680 [==============================] - 687s 4ms/step - loss: 2.5619\n",
      "\n",
      "Epoch 00006: loss improved from 2.60917 to 2.56193, saving model to weights_improvement_ 6_2.5619.hdf5\n",
      "Epoch 7/20\n",
      "163680/163680 [==============================] - 685s 4ms/step - loss: 2.5138\n",
      "\n",
      "Epoch 00007: loss improved from 2.56193 to 2.51382, saving model to weights_improvement_ 7_2.5138.hdf5\n",
      "Epoch 8/20\n",
      "163680/163680 [==============================] - 686s 4ms/step - loss: 2.4687\n",
      "\n",
      "Epoch 00008: loss improved from 2.51382 to 2.46874, saving model to weights_improvement_ 8_2.4687.hdf5\n",
      "Epoch 9/20\n",
      "163680/163680 [==============================] - 684s 4ms/step - loss: 2.4295\n",
      "\n",
      "Epoch 00009: loss improved from 2.46874 to 2.42947, saving model to weights_improvement_ 9_2.4295.hdf5\n",
      "Epoch 10/20\n",
      "163680/163680 [==============================] - 682s 4ms/step - loss: 2.3885\n",
      "\n",
      "Epoch 00010: loss improved from 2.42947 to 2.38848, saving model to weights_improvement_ 10_2.3885.hdf5\n",
      "Epoch 11/20\n",
      "163680/163680 [==============================] - 674s 4ms/step - loss: 2.3526\n",
      "\n",
      "Epoch 00011: loss improved from 2.38848 to 2.35261, saving model to weights_improvement_ 11_2.3526.hdf5\n",
      "Epoch 12/20\n",
      "163680/163680 [==============================] - 671s 4ms/step - loss: 2.3182\n",
      "\n",
      "Epoch 00012: loss improved from 2.35261 to 2.31823, saving model to weights_improvement_ 12_2.3182.hdf5\n",
      "Epoch 13/20\n",
      "163680/163680 [==============================] - 664s 4ms/step - loss: 2.2854\n",
      "\n",
      "Epoch 00013: loss improved from 2.31823 to 2.28544, saving model to weights_improvement_ 13_2.2854.hdf5\n",
      "Epoch 14/20\n",
      "163680/163680 [==============================] - 659s 4ms/step - loss: 2.2554\n",
      "\n",
      "Epoch 00014: loss improved from 2.28544 to 2.25536, saving model to weights_improvement_ 14_2.2554.hdf5\n",
      "Epoch 15/20\n",
      "163680/163680 [==============================] - 662s 4ms/step - loss: 2.2303\n",
      "\n",
      "Epoch 00015: loss improved from 2.25536 to 2.23034, saving model to weights_improvement_ 15_2.2303.hdf5\n",
      "Epoch 16/20\n",
      "163680/163680 [==============================] - 662s 4ms/step - loss: 2.2025\n",
      "\n",
      "Epoch 00016: loss improved from 2.23034 to 2.20253, saving model to weights_improvement_ 16_2.2025.hdf5\n",
      "Epoch 17/20\n",
      "163680/163680 [==============================] - 672s 4ms/step - loss: 2.1716\n",
      "\n",
      "Epoch 00017: loss improved from 2.20253 to 2.17157, saving model to weights_improvement_ 17_2.1716.hdf5\n",
      "Epoch 18/20\n",
      "163680/163680 [==============================] - 675s 4ms/step - loss: 2.1459\n",
      "\n",
      "Epoch 00018: loss improved from 2.17157 to 2.14594, saving model to weights_improvement_ 18_2.1459.hdf5\n",
      "Epoch 19/20\n",
      "163680/163680 [==============================] - 682s 4ms/step - loss: 2.1235\n",
      "\n",
      "Epoch 00019: loss improved from 2.14594 to 2.12350, saving model to weights_improvement_ 19_2.1235.hdf5\n",
      "Epoch 20/20\n",
      "163680/163680 [==============================] - 682s 4ms/step - loss: 2.0994\n",
      "\n",
      "Epoch 00020: loss improved from 2.12350 to 2.09940, saving model to weights_improvement_ 20_2.0994.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1fb319e39b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, \n",
    "         epochs=20, \n",
    "         batch_size= 128, \n",
    "         callbacks= [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'applying nlp basics.ipynb',\n",
       " 'data',\n",
       " 'day1.ipynb',\n",
       " 'day2.ipynb',\n",
       " 'day3.ipynb',\n",
       " 'day4.ipynb',\n",
       " 'GloVe  Global Vectors for Word Representation.html',\n",
       " 'nlp - basic',\n",
       " 'NLP overall review.ipynb',\n",
       " 'Untitled.ipynb',\n",
       " 'weights_improvement_ 10_2.3885.hdf5',\n",
       " 'weights_improvement_ 11_2.3526.hdf5',\n",
       " 'weights_improvement_ 12_2.3182.hdf5',\n",
       " 'weights_improvement_ 13_2.2854.hdf5',\n",
       " 'weights_improvement_ 14_2.2554.hdf5',\n",
       " 'weights_improvement_ 15_2.2303.hdf5',\n",
       " 'weights_improvement_ 16_2.2025.hdf5',\n",
       " 'weights_improvement_ 17_2.1716.hdf5',\n",
       " 'weights_improvement_ 18_2.1459.hdf5',\n",
       " 'weights_improvement_ 19_2.1235.hdf5',\n",
       " 'weights_improvement_ 1_2.9834.hdf5',\n",
       " 'weights_improvement_ 20_2.0994.hdf5',\n",
       " 'weights_improvement_ 2_2.8038.hdf5',\n",
       " 'weights_improvement_ 3_2.7255.hdf5',\n",
       " 'weights_improvement_ 4_2.6608.hdf5',\n",
       " 'weights_improvement_ 5_2.6092.hdf5',\n",
       " 'weights_improvement_ 6_2.5619.hdf5',\n",
       " 'weights_improvement_ 7_2.5138.hdf5',\n",
       " 'weights_improvement_ 8_2.4687.hdf5',\n",
       " 'weights_improvement_ 9_2.4295.hdf5',\n",
       " 'Word Vectors.ipynb']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    *Remove all punctuation from the source text, and therefore from the modelsâ€™ vocabulary.\n",
    "    *Try a one hot encoded for the input sequences.\n",
    "    *Train the model on padded sentences rather than random sequences of characters.\n",
    "    *Increase the number of training epochs to 100 or many hundreds.\n",
    "    *Add dropout to the visible input layer and consider tuning the dropout percentage.\n",
    "    *Tune the batch size, try a batch size of 1 as a (very slow) baseline and larger sizes from there.\n",
    "    *Add more memory units to the layers and/or more layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
