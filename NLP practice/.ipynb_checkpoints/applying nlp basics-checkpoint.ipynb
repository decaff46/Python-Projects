{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.zip')\n",
    "#test = pd.read_csv('data/test.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le= preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(train['author'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    1    2]\n",
      " [7900 5635 6044]]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EAP    7900\n",
       "MWS    6044\n",
       "HPL    5635\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the y has been converted correctly\n",
    "train['author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(train['text'].values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency - Inverse Document Frequency) with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=3, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
       "                smooth_idf=1, stop_words='english', strip_accents='unicode',\n",
       "                sublinear_tf=1, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                use_idf=1, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, \n",
    "                        max_features=None, \n",
    "                        strip_accents='unicode', \n",
    "                        analyzer='word', \n",
    "                        token_pattern=r'\\w{1,}',\n",
    "                        ngram_range=(1, 3), use_idf=1,\n",
    "                        smooth_idf=1,\n",
    "                        sublinear_tf=1,\n",
    "                        stop_words = 'english')\n",
    "\n",
    "tfidf.fit(list(x_train), list(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf =  tfidf.transform(x_train) \n",
    "x_val_tfidf = tfidf.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logloss with TFIDF: 0.569 %'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_tfidf= LogisticRegression(C= 1)#C = 1/λ for regularization\n",
    "\n",
    "log_tfidf.fit(x_train_tfidf, y_train)\n",
    "pred = log_tfidf.predict_proba(x_val_tfidf)\n",
    "\n",
    "f\"logloss with TFIDF: {round(multiclass_logloss(y_val, pred),3)} %\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(analyzer='word',\n",
    "                     token_pattern=r'\\w{1,}',\n",
    "                     ngram_range=(1, 3), \n",
    "                     stop_words = 'english')\n",
    "\n",
    "cv.fit(list(x_train), list(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cv =  cv.transform(x_train) \n",
    "x_val_cv = cv.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logloss with CounterVectorizer: 0.527 %'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cv= LogisticRegression(C= 1)#C = 1/λ for regularization\n",
    "\n",
    "log_cv.fit(x_train_cv, y_train)\n",
    "pred = log_cv.predict_proba(x_val_cv)\n",
    "\n",
    "f\"logloss with CounterVectorizer: {round(multiclass_logloss(y_val, pred),3)} %\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logloss with Naive Bayes on TFIDF: 0.578 %'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(x_train_tfidf, y_train)\n",
    "pred = nb.predict_proba(x_val_tfidf)\n",
    "\n",
    "f\"logloss with Naive Bayes on TFIDF: {round(multiclass_logloss(y_val, pred),3)} %\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes on CounterVerctorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logloss with Naive Bayes on CV: 0.487 %'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(x_train_cv, y_train)\n",
    "pred = nb.predict_proba(x_val_cv)\n",
    "\n",
    "f\"logloss with Naive Bayes on CV: {round(multiclass_logloss(y_val, pred),3)} %\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM on TFDIF\n",
    "\n",
    "Since SVMs take a lot of time, I reduce the number of features from the TF-IDF using Singular Value Decomposition before applying SVM. Also, I will do standardize the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out the 100 components\n",
    "svd = decomposition.TruncatedSVD(n_components=100)\n",
    "\n",
    "svd.fit(x_train_tfidf)\n",
    "x_train_svd = svd.transform(x_train_tfidf)\n",
    "x_val_svd = svd.transform(x_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize\n",
    "\n",
    "sc = preprocessing.StandardScaler()\n",
    "sc.fit(x_train_svd)\n",
    "x_train_svd_sc = sc.transform(x_train_svd)\n",
    "x_val_svd_sc = sc.transform(x_val_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=1.0, probability=True) \n",
    "svc.fit(x_train_svd_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logloss with SVM on TFIDF: 0.746 %'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = svc.predict_proba(x_val_svd_sc)\n",
    "\n",
    "f\"logloss with SVM on TFIDF: {round(multiclass_logloss(y_val, pred),3)} %\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM on CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_val_svc_sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-85caabd7bb8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_svd_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val_svc_sc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;34mf\"logloss with SVM on TFIDF: {round(multiclass_logloss(y_val, pred),3)} %\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_val_svc_sc' is not defined"
     ]
    }
   ],
   "source": [
    "svd.fit(x_train_cv)\n",
    "x_train_svd = svd.transform(x_train_cv)\n",
    "x_val_svd = svd.transform(x_val_cv)\n",
    "\n",
    "sc.fit(x_train_svd)\n",
    "x_train_svd_sc = sc.transform(x_train_svd)\n",
    "x_val_svd_sc = sc.transform(x_val_svd)\n",
    "\n",
    "svc = SVC(C=1.0, probability=True) \n",
    "svc.fit(x_train_svd_sc, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logloss with SVM on CV: 0.79 %'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = svc.predict_proba(x_val_svd_sc)\n",
    "\n",
    "f\"logloss with SVM on CV: {round(multiclass_logloss(y_val, pred),3)} %\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG Boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=200, n_jobs=10, nthread=10, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg = xgb.XGBClassifier(max_depth=7, \n",
    "                       n_estimators=200,\n",
    "                       colsample_bytree=0.8,\n",
    "                       subsample=0.8,\n",
    "                       nthread=10,\n",
    "                       learning_rate=0.1)\n",
    "\n",
    "xg.fit(x_train_svd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logloss with XGboost on CV decomposed: 0.799 %'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = xg.predict_proba(x_val_svd)\n",
    "\n",
    "f\"logloss with XGboost on CV decomposed: {round(multiclass_logloss(y_val, pred),3)} %\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, NB on Counter Vectorizer performed the best. What if I use Grid Search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring method\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "scores = metrics.make_scorer(multiclass_logloss, \n",
    "                            greater_is_better= False,\n",
    "                            needs_proba= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc = TruncatedSVD()\n",
    "sc = preprocessing.StandardScaler()\n",
    "lg = LogisticRegression()\n",
    "\n",
    "# pipeline\n",
    "mod_pipe= pipeline.Pipeline([('trunc', trunc),\n",
    "                          ('sc', sc),\n",
    "                          ('lg', lg)])\n",
    "\n",
    "# grid\n",
    "param_grid = {'trunc__n_components':[100,200],\n",
    "              'lg__C': [0.1, 1, 10],\n",
    "              'lg__penalty':['l1','l2']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:   22.0s remaining:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:   22.4s remaining:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:   29.2s remaining:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:   31.3s remaining:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   31.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   31.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.731\n",
      "Best parameters set:\n",
      "\tlg__C: 1\n",
      "\tlg__penalty: 'l2'\n",
      "\ttrunc__n_components: 200\n"
     ]
    }
   ],
   "source": [
    "model = GridSearchCV(estimator=mod_pipe,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring=scores,\n",
    "                     verbose=10,\n",
    "                     n_jobs=-1,\n",
    "                     iid= True,\n",
    "                     refit= True,\n",
    "                     cv=2)\n",
    "\n",
    "model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow, this got worse than original.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search in NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0598s.) Setting batch_size=6.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.492\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "mod_pipe = pipeline.Pipeline([('nb', nb)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=mod_pipe, \n",
    "                     param_grid=param_grid, \n",
    "                     scoring=scores,\n",
    "                     verbose=10,\n",
    "                     n_jobs=-1,\n",
    "                     iid=True,\n",
    "                     refit=True,\n",
    "                     cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(x_train_tfidf, y_train)  \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about 8% improvement from the previous NB on tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
