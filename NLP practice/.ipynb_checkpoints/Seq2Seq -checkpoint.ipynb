{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq French - Eng demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177210"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "lines = pd.read_csv('data/fra.txt', header= None, names = ['src', 'tar', 'att'], sep='\\t')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "      <th>att</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14158</th>\n",
       "      <td>He has no hat on.</td>\n",
       "      <td>Il ne porte pas de chapeau.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24389</th>\n",
       "      <td>I may die tomorrow.</td>\n",
       "      <td>Je mourrai peut-être demain.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21512</th>\n",
       "      <td>Tom lives near us.</td>\n",
       "      <td>Tom habite près de chez nous.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Let me in.</td>\n",
       "      <td>Laissez-moi rentrer.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5227</th>\n",
       "      <td>He's adorable.</td>\n",
       "      <td>Il est adorable.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29466</th>\n",
       "      <td>I drive a black car.</td>\n",
       "      <td>Je conduis une voiture noire.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45723</th>\n",
       "      <td>Who helps your mother?</td>\n",
       "      <td>Qui aide votre mère ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24787</th>\n",
       "      <td>I'll foot the bill.</td>\n",
       "      <td>Je paierai l'addition.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18900</th>\n",
       "      <td>I bought the book.</td>\n",
       "      <td>J'ai acheté le livre.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48105</th>\n",
       "      <td>I don't need an office.</td>\n",
       "      <td>Je n'ai pas besoin de bureau.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                            tar  \\\n",
       "14158        He has no hat on.    Il ne porte pas de chapeau.   \n",
       "24389      I may die tomorrow.   Je mourrai peut-être demain.   \n",
       "21512       Tom lives near us.  Tom habite près de chez nous.   \n",
       "840                 Let me in.           Laissez-moi rentrer.   \n",
       "5227            He's adorable.               Il est adorable.   \n",
       "29466     I drive a black car.  Je conduis une voiture noire.   \n",
       "45723   Who helps your mother?          Qui aide votre mère ?   \n",
       "24787      I'll foot the bill.         Je paierai l'addition.   \n",
       "18900       I bought the book.          J'ai acheté le livre.   \n",
       "48105  I don't need an office.  Je n'ai pas besoin de bureau.   \n",
       "\n",
       "                                                     att  \n",
       "14158  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "24389  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "21512  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "840    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "5227   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "29466  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "45723  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "24787  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "18900  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "48105  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take only 60K \n",
    "lines = lines[0:60000]\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34772</th>\n",
       "      <td>He's a computer nerd.</td>\n",
       "      <td>C'est un informaticien boutonneux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19054</th>\n",
       "      <td>I feel better now.</td>\n",
       "      <td>Je me sens mieux, maintenant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>Years passed.</td>\n",
       "      <td>Des années ont passé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4550</th>\n",
       "      <td>Was it funny?</td>\n",
       "      <td>Était-ce drôle ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15343</th>\n",
       "      <td>I'm an alcoholic.</td>\n",
       "      <td>Je suis alcoolique.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49269</th>\n",
       "      <td>I'm honored to be here.</td>\n",
       "      <td>Je suis honoré d'être ici.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13618</th>\n",
       "      <td>Are you on crack?</td>\n",
       "      <td>Êtes-vous droguée au crack ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49189</th>\n",
       "      <td>I'm afraid of the dark.</td>\n",
       "      <td>J'ai peur du noir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9687</th>\n",
       "      <td>Why can't I go?</td>\n",
       "      <td>Pourquoi est-ce que je ne peux pas y aller ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21070</th>\n",
       "      <td>The soup is thick.</td>\n",
       "      <td>La soupe est épaisse.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                                           tar\n",
       "34772    He's a computer nerd.            C'est un informaticien boutonneux.\n",
       "19054       I feel better now.                 Je me sens mieux, maintenant.\n",
       "4704             Years passed.                         Des années ont passé.\n",
       "4550             Was it funny?                              Était-ce drôle ?\n",
       "15343        I'm an alcoholic.                           Je suis alcoolique.\n",
       "49269  I'm honored to be here.                    Je suis honoré d'être ici.\n",
       "13618        Are you on crack?                  Êtes-vous droguée au crack ?\n",
       "49189  I'm afraid of the dark.                            J'ai peur du noir.\n",
       "9687           Why can't I go?  Pourquoi est-ce que je ne peux pas y aller ?\n",
       "21070       The soup is thick.                         La soupe est épaisse."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['src','tar']] # taking off the att\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13216</th>\n",
       "      <td>Why did you lie?</td>\n",
       "      <td>\\t Pourquoi as-tu menti ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36655</th>\n",
       "      <td>I've found a new job.</td>\n",
       "      <td>\\t J'ai trouvé un nouveau travail. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42324</th>\n",
       "      <td>I suppose that's true.</td>\n",
       "      <td>\\t Je suppose que c'est vrai. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>I want to do it.</td>\n",
       "      <td>\\t Je veux le faire. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27992</th>\n",
       "      <td>You're very clever.</td>\n",
       "      <td>\\t Vous êtes très ingénieuses. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23872</th>\n",
       "      <td>I am not a teacher.</td>\n",
       "      <td>\\t Je ne suis pas un enseignant. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55158</th>\n",
       "      <td>I know how busy you are.</td>\n",
       "      <td>\\t Je sais combien tu es occupé. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44899</th>\n",
       "      <td>Tom is a pediatrician.</td>\n",
       "      <td>\\t Tom est pédiatre. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48620</th>\n",
       "      <td>I promised not to tell.</td>\n",
       "      <td>\\t Je promis de ne rien dire. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18441</th>\n",
       "      <td>Have you finished?</td>\n",
       "      <td>\\t As-tu fini ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                                    tar\n",
       "13216          Why did you lie?           \\t Pourquoi as-tu menti ? \\n\n",
       "36655     I've found a new job.  \\t J'ai trouvé un nouveau travail. \\n\n",
       "42324    I suppose that's true.       \\t Je suppose que c'est vrai. \\n\n",
       "11302          I want to do it.                \\t Je veux le faire. \\n\n",
       "27992       You're very clever.      \\t Vous êtes très ingénieuses. \\n\n",
       "23872       I am not a teacher.    \\t Je ne suis pas un enseignant. \\n\n",
       "55158  I know how busy you are.    \\t Je sais combien tu es occupé. \\n\n",
       "44899    Tom is a pediatrician.                \\t Tom est pédiatre. \\n\n",
       "48620   I promised not to tell.       \\t Je promis de ne rien dire. \\n\n",
       "18441        Have you finished?                     \\t As-tu fini ? \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to add <sos> for initiating and <eos> for ending \n",
    "lines.tar = lines.tar.apply(lambda x: '\\t ' + x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabs: \n",
    "\n",
    "src_vocab=set()\n",
    "for line in lines.src: \n",
    "    for char in line: \n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어와 프랑스어는 각각 약 80개와 100개의 글자가 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, ',': 11, '-': 12, '.': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '\\xa0': 78, '«': 79, '»': 80, 'À': 81, 'Ç': 82, 'É': 83, 'Ê': 84, 'Ô': 85, 'à': 86, 'â': 87, 'ç': 88, 'è': 89, 'é': 90, 'ê': 91, 'ë': 92, 'î': 93, 'ï': 94, 'ô': 95, 'ù': 96, 'û': 97, 'œ': 98, 'С': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n"
     ]
    }
   ],
   "source": [
    "# applying index to char\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n"
     ]
    }
   ],
   "source": [
    "# encoder (eng): \n",
    "encoder_input = []\n",
    "for line in lines.src: \n",
    "    temp_x =[]\n",
    "    \n",
    "    for w in line: \n",
    "        temp_x.append(src_to_index[w]) # convert w to int\n",
    "        \n",
    "    encoder_input.append(temp_x)\n",
    "\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 47, 52, 3, 4, 3, 2], [1, 3, 44, 52, 63, 72, 71, 3, 4, 3, 2], [1, 3, 44, 52, 63, 72, 71, 13, 3, 2], [1, 3, 28, 66, 72, 69, 70, 104, 4, 3, 2], [1, 3, 28, 66, 72, 69, 56, 77, 104, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# decoder (fra):\n",
    "decoder_input =[]\n",
    "for line in lines.tar:\n",
    "    temp_x = []\n",
    "    \n",
    "    for w in line: \n",
    "        temp_x.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_x)\n",
    "\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더의 예측값과 비교하기 위한 실제값이 필요. 그런데 이 실제값에는 시작 심볼에 해당되는 sos 가 있을 필요가 없음! 즉, 처음 값 ('\\t')들 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 47, 52, 3, 4, 3, 2], [3, 44, 52, 63, 72, 71, 3, 4, 3, 2], [3, 44, 52, 63, 72, 71, 13, 3, 2], [3, 28, 66, 72, 69, 70, 104, 4, 3, 2], [3, 28, 66, 72, 69, 56, 77, 104, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t>0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t+=1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "# padding:\n",
    "\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병렬 데이터는 영어와 프랑스어의 길이는 하나의 쌍이라고 하더라도 전부 다르므로 패딩을 할 때도 이 두 개의 데이터의 길이를 전부 동일하게 맞춰줄 필요는 없음. 영어 데이터는 영어 샘플들끼리, 프랑스어는 프랑스어 샘플들끼리 길이를 맞추어서 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding: \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는데 decoder_input이 왜 필요 이유: \n",
    "\n",
    "    * 훈련 과정에서는 이전 시점의 디코더 셀 출력값을 현 시점 디코더 셀의 입력값으로 넣지 않고, 이전 시점의 실제값을 현 시점의 디코더 셀 값으로 사용! \n",
    "        - 이유: 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 함 --> 훈련 시간 저하\n",
    "        \n",
    "이 방법의 이름이 Teacher Forcing!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)# to pass the encoder's status to decoder: return_state = True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\caffr\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 291s 6ms/step - loss: 0.7715 - val_loss: 0.6948\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 284s 6ms/step - loss: 0.4820 - val_loss: 0.5507\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 373s 8ms/step - loss: 0.3989 - val_loss: 0.4873\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 367s 8ms/step - loss: 0.3532 - val_loss: 0.4441\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 321s 7ms/step - loss: 0.3235 - val_loss: 0.4198\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 282s 6ms/step - loss: 0.3021 - val_loss: 0.4012\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 280s 6ms/step - loss: 0.2856 - val_loss: 0.3897\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 296s 6ms/step - loss: 0.2725 - val_loss: 0.3799\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 311s 6ms/step - loss: 0.2613 - val_loss: 0.3721\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 289s 6ms/step - loss: 0.2519 - val_loss: 0.3673\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 280s 6ms/step - loss: 0.2436 - val_loss: 0.3629\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 291s 6ms/step - loss: 0.2364 - val_loss: 0.3592\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 290s 6ms/step - loss: 0.2297 - val_loss: 0.3585\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 317s 7ms/step - loss: 0.2239 - val_loss: 0.3573\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 295s 6ms/step - loss: 0.2184 - val_loss: 0.3558\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 307s 6ms/step - loss: 0.2132 - val_loss: 0.3557\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 300s 6ms/step - loss: 0.2085 - val_loss: 0.3557\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 309s 6ms/step - loss: 0.2040 - val_loss: 0.3540\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 318s 7ms/step - loss: 0.1999 - val_loss: 0.3560\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 294s 6ms/step - loss: 0.1957 - val_loss: 0.3568\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 302s 6ms/step - loss: 0.1921 - val_loss: 0.3587\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 292s 6ms/step - loss: 0.1885 - val_loss: 0.3608\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 293s 6ms/step - loss: 0.1851 - val_loss: 0.3633\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 301s 6ms/step - loss: 0.1819 - val_loss: 0.3659\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 295s 6ms/step - loss: 0.1789 - val_loss: 0.3643\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 297s 6ms/step - loss: 0.1759 - val_loss: 0.3674\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 293s 6ms/step - loss: 0.1731 - val_loss: 0.3683\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 293s 6ms/step - loss: 0.1704 - val_loss: 0.3721\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 297s 6ms/step - loss: 0.1680 - val_loss: 0.3724\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 285s 6ms/step - loss: 0.1654 - val_loss: 0.3744\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 283s 6ms/step - loss: 0.1631 - val_loss: 0.3779\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 280s 6ms/step - loss: 0.1608 - val_loss: 0.3801\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 280s 6ms/step - loss: 0.1587 - val_loss: 0.3809\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 286s 6ms/step - loss: 0.1566 - val_loss: 0.3846\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 398s 8ms/step - loss: 0.1544 - val_loss: 0.3875\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 399s 8ms/step - loss: 0.1525 - val_loss: 0.3903\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 400s 8ms/step - loss: 0.1507 - val_loss: 0.3940\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 397s 8ms/step - loss: 0.1489 - val_loss: 0.3956\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 398s 8ms/step - loss: 0.1471 - val_loss: 0.3970\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 398s 8ms/step - loss: 0.1455 - val_loss: 0.3993\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 365s 8ms/step - loss: 0.1438 - val_loss: 0.4027\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 282s 6ms/step - loss: 0.1422 - val_loss: 0.4037\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 278s 6ms/step - loss: 0.1407 - val_loss: 0.4062\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 278s 6ms/step - loss: 0.1392 - val_loss: 0.4092\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 278s 6ms/step - loss: 0.1377 - val_loss: 0.4112\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 279s 6ms/step - loss: 0.1365 - val_loss: 0.4147\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 278s 6ms/step - loss: 0.1350 - val_loss: 0.4158\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 287s 6ms/step - loss: 0.1339 - val_loss: 0.4184\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 281s 6ms/step - loss: 0.1325 - val_loss: 0.4205\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 280s 6ms/step - loss: 0.1313 - val_loss: 0.4235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13d73a9d6d8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying seq2seq translator\n",
    "Process:\n",
    "1. get the sentence's hidden and cell states that i want to traslate from encoder\n",
    "2. send status and sos to decoder\n",
    "3. until reaches \\n decoder do translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Input: Run!\n",
      "Correct senetence:  Cours ! \n",
      "Translated sentence:  Cours ! \n",
      "-----------------------------------\n",
      "Input: I lied.\n",
      "Correct senetence:  J'ai menti. \n",
      "Translated sentence:  J'ai appris l'argent. \n",
      "-----------------------------------\n",
      "Input: Come in.\n",
      "Correct senetence:  Entre. \n",
      "Translated sentence:  Entre. \n",
      "-----------------------------------\n",
      "Input: Hurry up.\n",
      "Correct senetence:  Magnez-vous ! \n",
      "Translated sentence:  Magne-toi ! \n",
      "-----------------------------------\n",
      "Input: We walked.\n",
      "Correct senetence:  Nous sommes allées à pied. \n",
      "Translated sentence:  Nous sommes amis. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # sentence index\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('Input:', lines.src[seq_index])\n",
    "    print('Correct senetence:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) \n",
    "    print('Translated sentence:', decoded_sentence[:len(decoded_sentence)-1]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
